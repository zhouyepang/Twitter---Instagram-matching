{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stylext: Tweet Attribution with Naive Bayes & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to non-programmer friends: *You need to select \"Restart & Clear Output\" under the Kernel tab before running the code cells below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Although technically not 100% pure stylometry (because distinguishing one user from the other with the code below is affected by the respective topics being discussed), this notebook file will illustrate how the same sort of algorithms used to distinguish spam from valid email can also be used to distinguish one Twitter user from another using their post content alone.\n",
    "\n",
    "Both feeds used in the sample csv data are about the same topic (economics). However, there are no *conscious* attempts by the users to obfuscate their Tweet styles, but the techniques used can still be useful if someone is unaware of what distinguishes their tweet style from others.\n",
    "\n",
    "With each Python code cell, click on it to highlight then shift + enter to execute it. The * symbol means it's running, while a number means it completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Importing Needed Libraries\n",
    "\n",
    "You will need *pandas* to read in rows and colums (containing the raw article text, and columns for all of the criteria of interest.\n",
    "\n",
    "*Numpy* and *scipy* add functionality that you will depend on throughout notebook use. Very specific tools are also imported from *scikit-learn.* In particular, a few natural language processing tools are imported which may be used to boost model accuracy (with iterative trial and error).\n",
    "\n",
    "**Do not worry if the brief library descriptions in the code below do not make sense to you; the specifics of what they do are ellaborated further in this notebook as they are put to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the core libraries you need to import to run the scripts that follow.\n",
    "\n",
    "import pandas as pd # this is needed to read in dataframes (rows and columns of data)\n",
    "import numpy as np # numpy allows you to efficiently work with and execute operations on arrays\n",
    "import scipy as sp # scipy builds off of numpy, giving you added linear algebra functionality under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our core libraries are imported, we need to import several things from Scikit-Learn. These will allow use to *add structure* to otherwise unstructured text, *apply machine learning models* to classify text samples, and *measure the accuracy* of the output for the data we will load in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are more specific tools from Scikit-Learn for natural language processing and measuring accuracy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # two vectorization methods we want for later\n",
    "from sklearn.naive_bayes import MultinomialNB # multinomial naive bayes classifier\n",
    "from sklearn.linear_model import LogisticRegression # basic logistic regression classifier\n",
    "from sklearn.model_selection import train_test_split # this splits the data loaded in into training & testing groups\n",
    "from sklearn import metrics # this will help us understand the results of the train/test split simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load in CSV File Containing Tweets and Define Train/Test Variables\n",
    "\n",
    "Now we will read in the data file (in comma seperated values format) from an online github repo, and store it in a python variable called \"tweets\" so we can continue to work with it throughout the notebook. Pandas' \"read_csv\" feature will allow use to read in and define the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read post_feed.csv into a DataFrame. Any CSV with columns containing raw tweet contents and usernames can often work.\n",
    "# If you're offline, replace the link with the file location for post_feed.csv if you have it stored locally.\n",
    "\n",
    "url = 'csv/10users.csv' # define url as csv data\n",
    "tweets = pd.read_csv(url) # read the csv file using the pandas python library and define it as 'tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining training and testing variables must be done in order to even begin testing and improving predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define X and y, or the manipulated variable and the responding variable: Given the text, which user tweeted it?\n",
    "\n",
    "X = tweets.raw_text  # this defines X as the csv column that contains all the raw tweet text of both users\n",
    "y = tweets.username  # the responding variable (y) is now defined as the column with the two Twitter usernames\n",
    "\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below ensures the dataframe has been read in and the variables are defined as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DwyaneWade</td>\n",
       "      <td>b'I\\xe2\\x80\\x99ll take each game just like thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DwyaneWade</td>\n",
       "      <td>b'https://t.co/CDaW9lo7dp'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DwyaneWade</td>\n",
       "      <td>b'RT @RobinRoberts: It was a show-stopping per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DwyaneWade</td>\n",
       "      <td>b'I guess all black people do look alike \\xf0\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DwyaneWade</td>\n",
       "      <td>b'Happy Anniversary to you both!!! https://t.c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     username                                           raw_text\n",
       "0  DwyaneWade  b'I\\xe2\\x80\\x99ll take each game just like thi...\n",
       "1  DwyaneWade                         b'https://t.co/CDaW9lo7dp'\n",
       "2  DwyaneWade  b'RT @RobinRoberts: It was a show-stopping per...\n",
       "3  DwyaneWade  b'I guess all black people do look alike \\xf0\\...\n",
       "4  DwyaneWade  b'Happy Anniversary to you both!!! https://t.c..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # check the first five rows/tweets\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27098</th>\n",
       "      <td>wojespn</td>\n",
       "      <td>b'Sources: Brooklyn is out of the running on f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27099</th>\n",
       "      <td>wojespn</td>\n",
       "      <td>b'@ChrisMannixYS @KDTrey5 Too bad KD plays for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27100</th>\n",
       "      <td>wojespn</td>\n",
       "      <td>b'RT @TheVertical: Sources: Arron Afflalo agre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27101</th>\n",
       "      <td>wojespn</td>\n",
       "      <td>b'RT @BobbyMarks42: Kings projected cap space ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27102</th>\n",
       "      <td>wojespn</td>\n",
       "      <td>b'Arron Afflalo has agreed to a two-year, $25M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username                                           raw_text\n",
       "27098  wojespn  b'Sources: Brooklyn is out of the running on f...\n",
       "27099  wojespn  b'@ChrisMannixYS @KDTrey5 Too bad KD plays for...\n",
       "27100  wojespn  b'RT @TheVertical: Sources: Arron Afflalo agre...\n",
       "27101  wojespn  b'RT @BobbyMarks42: Kings projected cap space ...\n",
       "27102  wojespn  b'Arron Afflalo has agreed to a two-year, $25M..."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the last five rows/tweets, notice the change in which username's tweets are visible\n",
    "\n",
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27103, 2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of rows (tweets stored) and columns\n",
    "\n",
    "tweets.shape\n",
    "\n",
    "# even though there are 3 columns, most of the time we're only going to use two at a time (no need for time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'I\\xe2\\x80\\x99ll take each game just like thi...\n",
       "1                           b'https://t.co/CDaW9lo7dp'\n",
       "2    b'RT @RobinRoberts: It was a show-stopping per...\n",
       "3    b'I guess all black people do look alike \\xf0\\...\n",
       "4    b'Happy Anniversary to you both!!! https://t.c...\n",
       "Name: raw_text, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first five rows in a shorter format - it's only displaying the raw_text column that X was defined as.\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DwyaneWade\n",
       "1    DwyaneWade\n",
       "2    DwyaneWade\n",
       "3    DwyaneWade\n",
       "4    DwyaneWade\n",
       "Name: username, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will see the first five rows of the responding variable (the username of who posted what)\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Time to Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Separate text into units such as sentences or words that can be better quantified\n",
    "- **Why:** Gives structure to previously unstructured text that machine learning can be applied to\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "We are now going to create what are called \"document-term matrices\" of the tweets. Think of these as rows and columns which store numbers representing how often certain terms appear in a given document (or passage of text). The image below may help you understand what that looks like under the hood:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![Document-Term Matrix](http://mlg.postech.ac.kr/static/research/nmf_cluster1.PNG)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "\n",
    "vect = CountVectorizer() # because vect is way easier to type than CountVectorizer...\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.fit_transform(X_test)\n",
    "\n",
    "# now we have quantitative info about the tweets that a 'multinomial naive Bayes classifier' can work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just to clarify what's going on in the adjacent cells:** All the **rows** are the *individual tweets* that are stored in the CSV file. But the astronomical crapload of **columns** is literally *each unique term* that appears. Those are going to be the \"features\" used to \"fingerprint\" one user from another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20327, 31449)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zn6trwryhi', 'zne', 'znianvui01', 'zo', 'zo2_', 'zo2lft2fhf', 'zoehan06090', 'zombie', 'zombies', 'zone', 'zones', 'zonkerdk', 'zonwdofjxf', 'zookeeper', 'zoolander', 'zoom', 'zoran_dragic', 'zosh7milj2', 'zpjwgn', 'zpydnmvv2n', 'zq7lirqyoj', 'zq8dprgk0f', 'zr', 'zrgvdvqnaf', 'zrrcisbnyz', 'zryl3kro1m', 'zs718ifpiy', 'zslkdfic', 'zsoldrofou', 'zsu9qc8dr1', 'zsxmxo9kqs', 'ztemupbtij', 'ztsbk0nzhb', 'zu1vo8woqy', 'zug0ncxmew', 'zuma', 'zusswrvq9u', 'zv50tc8ahl', 'zv91pwsgvr', 'zvwwvexg7h', 'zvzy2mudlw', 'zw9rfoqmcq', 'zwdrt26nmt', 'zwjthcryau', 'zxfodksdsq', 'zxfoyjg5vt', 'zynhggzxso', 'zyygjlthdg', 'zzuv1ayiyr', 'zzzz']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options, which are currently at their default values\n",
    "\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the output above.** These are settings that CountVectorizer has (which is currently stored in \"vect\"). The main ones to keep in mind are lowercase (whether all the words get converted to lowercase), max_features (how many of those words are used to \"fingerprint\" the text of the tweets), ngram_range (if set to (1, 2), it will look at individual words as well as word pairs, and so on as you increment the latter number), and stop_words (words that are so common that they might not be useful for tweet classification can be ignored).\n",
    "\n",
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) - in case you might be interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20327, 36404)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will not convert to lowercase for now, but if we did it would reduce the number of unique words looked at\n",
    "\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameter **lowercase:** boolean, True by default\n",
    "    - If True, Convert all characters to lowercase before tokenizing.\n",
    "    \n",
    "This can be useful for preventing word capitalization from making your results less predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zgloIYgjV8', 'zgq0PajuSD', 'ziByLyfeKr', 'ziJUvzh8ks', 'ziKqmXMpkp', 'ziggurat', 'zigzaganalytics', 'zirvin21', 'zjRgmNjGBc', 'zjqLWzyWSZ', 'zkCkzNUpO5', 'zkT4KokYhS', 'zkvqUmSw', 'zlah2ov2Ty', 'zmB9YgJ9kA', 'zmane2', 'zmbiAtURsF', 'zmfHPnFDDj', 'zoNhzB7RMD', 'zombies', 'zone', 'zones', 'zoolander', 'zoomed', 'zoop', 'zoran_dragic', 'zoykWGGtwG', 'zplc1y5NQH', 'zpoRQzqjRF', 'zq72pYXw9x', 'zqQ7loGnE8', 'zquPwXRvxn', 'zquadri90', 'zrDUJukU', 'zrxUeJIyvQ', 'zsKF5HdiBS', 'zsQzDKn6', 'ztKX6JH2fy', 'ztPHrAlhLL', 'ztYAMYpbxb', 'ztsp8TUMUn', 'zufIuarf', 'zukovka', 'zupWf2Gxql', 'zvILQN9S', 'zw4DaT34MT', 'zxRPHDa7AR', 'zynga', 'zzZz', 'zzz']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20327, 168356)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameter **ngram_range:** tuple (min_n, max_n)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zuwhm6hiao', 'zuz5segxgh', 'zuz5segxgh https', 'zv6cpa9po6', 'zveeqrjpu2', 'zvilqn9s', 'zvl23bexir', 'zw4dat34mt', 'zwade', 'zwade what', 'zwazbpljyj', 'zwhueysw0l', 'zwischenzug', 'zwjm1bih47', 'zwomuphxnt', 'zwomuphxnt https', 'zxbjh2sbgr', 'zxfoyjg5vt', 'zxihl69ga5', 'zxmczel8', 'zxrphda7ar', 'zxsxfixu', 'zxsxfixu xe2', 'zxuvqys3ps', 'zxvjpiep6k', 'zxvwo28nay', 'zxxvkjvm', 'zyg_26', 'zyg_26 lol', 'zygrqqbfwm', 'zynga', 'zynga poker', 'zytm8cr5yt', 'zyuutsailv', 'zz2zwmw8j3', 'zz2zwmw8j3 xe2', 'zz46znjvyy', 'zzblosoqmj', 'zzly7stiny', 'zznxe7pr76', 'zzo39r7ggy', 'zzt3', 'zzt3 xe2', 'zztwdqd2fu', 'zzxnr0qtt7', 'zzz', 'zzz and', 'zzzz', 'zzzz https', 'zzzzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting which user made what Tweet:** \n",
    "\n",
    "Now for the moment of truth... How accurate can we predict who is who?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659238488784\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cell below will eliminate the need for typing in the same code over and over again, as well as produce an output that includes all the information we need to know about how the number of unique features is affecting the classifier accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "def tokenize_test(vect, model):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    if model == 'lr':\n",
    "        lr.fit(X_train_dtm, y_train)\n",
    "        y_pred_class = lr.predict(X_test_dtm)\n",
    "        algorithm = 'Logistic Regression'\n",
    "    elif model == 'nb':\n",
    "        nb.fit(X_train_dtm, y_train)\n",
    "        y_pred_class = nb.predict(X_test_dtm)\n",
    "        algorithm = 'Multinomial Naive Bayes'\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "    print(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  31449\n",
      "Accuracy:  0.723288075561\n",
      "Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect, model='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  31449\n",
      "Accuracy:  0.659238488784\n",
      "Multinomial Naive Bayes\n",
      "Features:  168356\n",
      "Accuracy:  0.636806375443\n",
      "Multinomial Naive Bayes\n",
      "Features:  31167\n",
      "Accuracy:  0.67458677686\n",
      "Multinomial Naive Bayes\n",
      "Features:  146877\n",
      "Accuracy:  0.668683589138\n",
      "Multinomial Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer()\n",
    "tokenize_test(vect, model='nb')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect, model='nb')\n",
    "\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect, model='nb')\n",
    "\n",
    "vect = CountVectorizer(stop_words='english',ngram_range=(1, 2))\n",
    "tokenize_test(vect, model='nb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
